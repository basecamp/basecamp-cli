# Skill Benchmark Spec

Empirical test to determine: does a minimal skill add value beyond `bcq --help`?

## Hypothesis

A minimal, CLI-generated skill provides value through:
1. **Domain invariants** that models consistently get wrong
2. **Standardized packaging** for discoverability and distribution

If the skill doesn't measurably improve outcomes, we rely solely on `bcq --help`.

## Conditions

| Condition | Skill | CLI | What agent sees |
|-----------|-------|-----|-----------------|
| **A** | `basecamp` | bcq | Generated skill with invariants + bcq CLI |
| **B** | none | bcq | "use `bcq --help`" prompt only |
| **C** | `basecamp-raw` | curl | Raw API skill with endpoint docs |

### Condition A: bcq + minimal skill

Agent receives:
- `benchmarks/skills/basecamp/SKILL.md` (generated by `bcq help --agent --format=skill`)
- Access to `bcq` CLI
- Implicit permission to run `bcq --help` for discovery

The skill provides:
- Domain invariants (bucket=project, todoset vs todolist, etc.)
- Preferred patterns (use bcq, not raw curl)
- Anti-patterns to avoid

### Condition B: bcq only

Agent receives:
- `benchmarks/skills/bcq-only/SKILL.md` — minimal "use bcq --help" prompt
- Access to `bcq` CLI
- No domain invariants

This tests whether `bcq --help` is self-sufficient.

### Condition C: raw API skill

Agent receives:
- `benchmarks/skills/basecamp-raw/SKILL.md` with API endpoint documentation
- curl + jq tools
- No bcq CLI

The raw skill provides:
- Authentication pattern
- Endpoint URLs and curl examples
- Response structure documentation
- Pagination and rate limiting notes

This is the baseline to measure bcq's value-add.

## Tasks

Reuse tasks from `benchmarks/spec.yaml` (the existing bcq vs raw benchmark):

| Task | Tests |
|------|-------|
| 01: List todos (paginated) | Pagination handling |
| 02: Find + complete todo | Chained operations |
| 03: Create + assign todo | Person lookup, assignment |
| 04: Comment on message | Recording types |
| 05: Reorder todo | Position API |
| 06: Create list + todos | Todoset vs todolist confusion |
| 07: Recover from 429 | Error handling |
| 08: Recover from 401 | Auth refresh |
| 09: Bulk complete overdue | Filter + bulk ops |
| 10: Cross-project search | Search API |

### Key differentiator tasks

Task 06 (Create list + todos) specifically tests todoset vs todolist confusion — a domain invariant in the skill.

If Condition A outperforms B on task 06, the invariants have value.

## Metrics

| Metric | Description |
|--------|-------------|
| `success_rate` | % of tasks completed correctly |
| `error_count` | Client errors (4xx) during task |
| `invariant_violations` | Times agent made a domain error (bucket/todoset confusion) |
| `help_invocations` | Times agent ran `bcq --help` or `bcq <cmd> --help` |
| `time_to_success_ms` | Time from task start to validation pass |

## Execution

```bash
# Run all three conditions
./benchmarks/harness.sh --condition skill     # Condition A: bcq + skill
./benchmarks/harness.sh --condition bcq       # Condition B: bcq only
./benchmarks/harness.sh --condition raw       # Condition C: raw API skill

# Compare results
jq -s 'group_by(.condition) | map({
  condition: .[0].condition,
  success_rate: ([.[] | select(.success)] | length) / length,
  avg_errors: ([.[] | .metrics.error_count] | add / length),
  avg_time_ms: ([.[] | .metrics.time_ms] | add / length)
})' benchmarks/results/*.json
```

## Decision Criteria

### Primary question: Does bcq help?

| Outcome | Decision |
|---------|----------|
| A or B >> C | bcq adds value, keep CLI |
| A or B ≈ C | bcq not worth it, raw API is fine |

### Secondary question: Does the skill add value beyond bcq --help?

| Outcome | Decision |
|---------|----------|
| A > B by ≥10% success rate | Keep skill, invariants help |
| A ≈ B (within 5%) | Drop skill, `bcq --help` is sufficient |
| A < B | Skill is harmful, drop it |

### Decision matrix

| A vs B | A/B vs C | Action |
|--------|----------|--------|
| A >> B | A >> C | Ship bcq + skill |
| A ≈ B | A ≈ B >> C | Ship bcq, drop skill |
| A ≈ B | A ≈ B ≈ C | Raw API sufficient, bcq optional |
| A < B | — | Skill harmful, investigate |

If A wins (skill adds value):
```bash
# Promote to official skill
mv benchmarks/skills/basecamp skills/basecamp
git add skills/basecamp

# Add to installer:
bcq help --agent --format=skill > ~/.config/basecamp/skills/basecamp/SKILL.md
```

## Files

| File | Purpose |
|------|---------|
| `lib/agent_invariants.json` | Source of truth for domain invariants |
| `benchmarks/skills/basecamp/SKILL.md` | Condition A: bcq + invariants |
| `benchmarks/skills/bcq-only/SKILL.md` | Condition B: bcq only |
| `benchmarks/skills/basecamp-raw/SKILL.md` | Condition C: raw API |
| `bcq help --agent` | Human-readable agent help |
| `bcq help --agent --format=skill` | Skill generator |
| `bcq help --agent --format=json` | Machine-readable for tooling |

## Promotion Strategy

All skills live in `benchmarks/skills/` until the benchmark decides.

After benchmark:
1. **If A wins**: Move `benchmarks/skills/basecamp/` to `skills/basecamp/`
2. **If B wins**: No skill shipped, rely on `bcq --help`
3. **If C wins**: bcq provides no value, reconsider CLI approach

The promoted skill becomes the bootstrap for distribution.

## Regenerating the Skill

When invariants change:
```bash
bcq help --agent --format=skill > benchmarks/skills/basecamp/SKILL.md
git add benchmarks/skills/basecamp/SKILL.md lib/agent_invariants.json
git commit -m "Update skill with new invariants"
```

The skill header includes:
```
# GENERATED by bcq X.Y.Z — do not edit
# Regenerate with: bcq help --agent --format=skill
```
